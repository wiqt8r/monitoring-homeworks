# Домашнее задание к занятию 17 «Инцидент-менеджмент»

# Постмортем: деградация сервисов GitHub 21–22 октября 2018
## Краткое резюме (Executive Summary)

21 октября 2018 года в 22:52 UTC GitHub столкнулся с деградацией сервиса, продолжавшейся 24 часа 11 минут. Пользователи наблюдали устаревшие/несогласованные данные на сайте (например, проблемы с отображением issues, pull requests, репозиториев), а часть функций (webhooks, Pages) была недоступна или работала с задержками.
Главная причина: кратковременный сетевой разрыв (43 секунды) во время плановых работ привёл к нарушению репликации и рассогласованию топологии кластеров MySQL, после чего восстановление целостности данных потребовало длительных ручных действий и переразвертывания реплик.
Потери пользовательских данных не произошло, но потребовалась длительная сверка и восстановление нескольких секунд операций записи. 

--

2) Влияние (Impact)
Что было затронуто

GitHub.com: отображение данных стало непоследовательным (часть данных — “как будто из прошлого”), периодами наблюдались ошибки. 
The GitHub Blog

Webhooks / GitHub Pages / внутренние системы обработки: были приостановлены или существенно деградировали, чтобы не усугубить рассогласование. 
The GitHub Blog
+1

Пользовательский эффект

Невозможность полагаться на актуальность информации на платформе.

Задержки/отсутствие автоматизаций (webhooks) → влияние на CI/CD у клиентов.

Замедление разработки и релизов у компаний, зависимых от GitHub.

Длительность

24 часа 11 минут деградации. 
The GitHub Blog
+1

3) Детекция (Detection)

Инцидент проявился через:

рост ошибок и жалоб пользователей;

внутренние метрики деградации запросов к данным и несогласованности в слоях чтения/кэша.

Официальный разбор подчёркивает, что изначальная проблема была сетевой, но развитие инцидента стало заметно по последствиям в БД и пользовательском интерфейсе. 
The GitHub Blog

4) Хронология (Timeline, UTC)

Ниже — укрупнённая хронология (под учебный формат), без перечисления каждой внутренней операции.

21.10 22:52 — плановые работы по замене сетевого оборудования; происходит разрыв связи между дата-центрами примерно на 43 секунды. 
Habr
+1

22:52–23:xx — система управления топологией и репликацией начинает перестраивать роли/лидеров БД; часть кластеров оказывается в “неожиданной” конфигурации. 
The GitHub Blog

Ночь 21→22.10 — GitHub переводит сервис в режим защиты целостности данных: ограничивает операции, приостанавливает webhooks/Pages и другие фоновые обработчики. 
The GitHub Blog
+1

22.10 (утро-день) — команда восстанавливает корректную репликацию и выбирает путь восстановления через согласование данных и “приведение к единому источнику истины”. 
The GitHub Blog

22.10 (вечер) — основная функциональность возвращается в норму; завершается деградация. 
The GitHub Blog

5) Корневая причина (Root Cause)

Непосредственная причина:
Короткий сетевой разрыв во время плановых работ вызвал сетевое разделение (network partition) между компонентами инфраструктуры GitHub.

Почему это привело к длительной деградации:

При сетевом разделении системы оркестрации и репликации БД начали переизбирать лидеров и менять роли реплик.

В результате возникло состояние, когда часть операций записи оказалась не полностью реплицирована между регионами/узлами, а чтение пользователям могло обслуживаться из источников с разной “верностью” данным.

GitHub сознательно выбрал приоритет целостности данных над быстрейшим восстановлением полного функционала, из-за чего пришлось остановить ряд систем и проводить восстановление осторожно и постепенно. 
The GitHub Blog

6) Сопутствующие факторы (Contributing Factors)

Сложность распределённой топологии БД и автоматического failover в условиях редкого, но возможного split-brain/partition сценария. 
The GitHub Blog

Недостаточная “предсказуемость” поведения оркестратора/процедур при очень коротких сетевых сбоях — 43 секунды оказалось достаточно для цепочки автоматических действий. 
The GitHub Blog
+1

Зависимости: при деградации данных пришлось останавливать дополнительные подсистемы (hooks, Pages), чтобы не усугублять состояние. 
The GitHub Blog
+1

7) Реакция и восстановление (Response & Recovery)
Ключевые действия команды

Быстрое распознавание риска потери целостности данных.

Приостановка подсистем, которые могли увеличивать расхождения (фоновые записи, webhooks, Pages). 
The GitHub Blog
+1

Выбор стратегии восстановления, которая минимизирует риск потери данных даже ценой длительного даунтайма.

Долгий процесс “сведения” данных и восстановления репликации.

Что было сделано правильно (What went well)

Приоритет: data integrity first (решение, хоть и болезненное для доступности, но правильное для доверия). 
The GitHub Blog

Публичная коммуникация: GitHub выпустил подробный разбор и признал влияние на пользователей. 
The GitHub Blog

Что пошло не так (What went wrong)

Автоматические механизмы failover/оркестрации не предотвратили сложное рассогласование.

Сценарии реагирования на “короткий сетевой разрыв” оказались недостаточно отработанными.

8) Уроки (Lessons Learned)

Даже секундные сетевые сбои в распределённой системе могут приводить к каскадным последствиям. 
forpes.ru
+1

Автоматический failover без строгих гарантий/гейтов может ухудшить ситуацию — особенно при сложной репликации. 
The GitHub Blog

Важно иметь готовые процедуры для выбора между:

быстрым восстановлением доступности,

и восстановлением целостности данных (GitHub выбрал второе). 
The GitHub Blog

9) План предотвращения (Action Items)

Ниже — “учебный” список корректирующих действий в духе официального разбора (без претензии на полный internal-roadmap):

Архитектура и устойчивость

Улучшить механизмы обнаружения и обработки network partition.

Усилить гарантии консенсуса/ролей лидера при межрегиональных сбоях. 
The GitHub Blog

Процедуры и автоматизация

Пересмотреть автоматические сценарии failover, добавить ограничения/ручные “предохранители” для редких сценариев.

Регулярно проводить game days / учения по разделению сети и рассинхронизации реплик.

Наблюдаемость и коммуникация

Улучшить метрики “консистентности данных” как first-class сигнал.

Уточнить пользовательские статусы и сообщения: “данные могут быть устаревшими” (это критично, когда сервис формально “работает”, но показывает неправильное). 
The GitHub Blog

10) Итог

Инцидент GitHub (21–22 октября 2018) — показательный пример того, как небольшой инфраструктурный сбой (43 секунды сетевой потери связности) может привести к суточной деградации из-за сложности распределённых систем и требований к целостности данных.
Ключевой управленческий выбор GitHub — сохранить доверие к данным, даже если это увеличивает время восстановления
