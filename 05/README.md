# Домашнее задание к занятию 17 «Инцидент-менеджмент»

# Постмортем: деградация сервисов GitHub 21–22 октября 2018
## Краткое резюме (Executive Summary)

21 октября 2018 года в 22:52 UTC GitHub столкнулся с деградацией сервиса, продолжавшейся 24 часа 11 минут. Пользователи наблюдали устаревшие/несогласованные данные на сайте (например, проблемы с отображением issues, pull requests, репозиториев), а часть функций (webhooks, Pages) была недоступна или работала с задержками.
Главная причина: кратковременный сетевой разрыв (43 секунды) во время плановых работ привёл к нарушению репликации и рассогласованию топологии кластеров MySQL, после чего восстановление целостности данных потребовало длительных ручных действий и переразвертывания реплик.
Потери пользовательских данных не произошло, но потребовалась длительная сверка и восстановление нескольких секунд операций записи. 

## Влияние (Impact)

Что было затронуто:
- GitHub.com: отображение данных стало непоследовательным (часть данных как будто из прошлого), периодами наблюдались ошибки. 
- Webhooks / GitHub Pages / внутренние системы обработки: были приостановлены или существенно деградировали, чтобы не усугубить рассогласование. 

Пользовательский опыт:
- Невозможность полагаться на актуальность информации на платформе.
- Задержки/отсутствие автоматизаций (webhooks) → влияние на CI/CD у клиентов.
- Замедление разработки и релизов у компаний, зависимых от GitHub.

Длительность
- 24 часа 11 минут деградации. 

## Обнаружение (Detection)

Инцидент был выявлен при помощи:
- роста ошибок и жалоб пользователей;
- внутренних метрик деградации запросов к данным и несогласованности в слоях чтения/кэша.

Официальный разбор подчёркивает, что изначальная проблема была сетевой, но развитие инцидента стало заметно по последствиям в БД и пользовательском интерфейсе. 

## Хронология (Timeline)

- 21.10 22:52 - плановые работы по замене сетевого оборудования. Происходит разрыв связи между дата-центрами примерно на 43 секунды. 
- 21.10 22:52-23:xx - система управления топологией и репликацией начинает перестраивать роли/лидеров БД, часть кластеров оказывается в "неожиданной" конфигурации. 
- Ночь 21-22.10 - GitHub переводит сервис в режим защиты целостности данных: ограничивает операции, приостанавливает webhooks/Pages и другие фоновые обработчики. 
- 22.10 (утро-день) - команда восстанавливает корректную репликацию и выбирает путь восстановления через согласование данных и "приведение к единому источнику истины". 
- 22.10 (вечер) — основная функциональность возвращается в норму, завершается деградация. 

## Корневая причина (Root Cause)

Непосредственная причина:
- Короткий сетевой разрыв во время плановых работ вызвал сетевое разделение (network partition) между компонентами инфраструктуры GitHub.

Почему это привело к длительной деградации:
- При сетевом разделении системы оркестрации и репликации БД начали переизбирать лидеров и менять роли реплик.
- В результате возникло состояние, когда часть операций записи оказалась не полностью реплицирована между регионами/узлами, а чтение пользователям могло обслуживаться из источников с разной верностью данным.
- GitHub сознательно выбрал приоритет целостности данных над быстрейшим восстановлением полного функционала, из-за чего пришлось остановить ряд систем и проводить восстановление осторожно и постепенно. 

## Сопутствующие факторы (Contributing Factors)

- Сложность распределённой топологии БД и автоматического failover в условиях редкого, но возможного split-brain/partition сценария. 
- Недостаточная "предсказуемость" поведения оркестратора/процедур при очень коротких сетевых сбоях — 43 секунды оказалось достаточно для цепочки автоматических действий. 
- Зависимости: при деградации данных пришлось останавливать дополнительные подсистемы (hooks, Pages), чтобы не усугублять состояние. 

## Реакция и восстановление (Response & Recovery)
Ключевые действия команды
- Быстрое распознавание риска потери целостности данных.
- Приостановка подсистем, которые могли увеличивать расхождения (фоновые записи, webhooks, Pages). 
- Выбор стратегии восстановления, которая минимизирует риск потери данных даже ценой длительного даунтайма.
- Долгий процесс "сведения" данных и восстановления репликации.

Что было сделано правильно (What went well)
- Приоритет: data integrity first (решение, хоть и болезненное для доступности, но правильное для доверия).
- Публичная коммуникация: GitHub выпустил подробный разбор и признал влияние на пользователей. 

Что пошло не так (What went wrong)
- Автоматические механизмы failover/оркестрации не предотвратили сложное рассогласование.
- Сценарии реагирования на короткий сетевой разрыв оказались недостаточно отработанными.

## Уроки (Lessons Learned)

- Даже секундные сетевые сбои в распределённой системе могут приводить к каскадным последствиям. 
- Автоматический failover без строгих гарантий/гейтов может ухудшить ситуацию — особенно при сложной репликации. 
- Важно иметь готовые процедуры для выбора между быстрым восстановлением доступности и восстановлением целостности данных (GitHub выбрал второе). 

## План предотвращения (Action Items)

Архитектура и устойчивость
- Улучшить механизмы обнаружения и обработки network partition.
- Усилить гарантии консенсуса/ролей лидера при межрегиональных сбоях. 

Процедуры и автоматизация
- Пересмотреть автоматические сценарии failover, добавить ограничения/ручные предохранители для редких сценариев.
- Регулярно проводить учения по разделению сети и рассинхронизации реплик.

Наблюдаемость и коммуникация
- Улучшить метрики консистентности данных как first-class сигнал.
- Уточнить пользовательские статусы и сообщения: "данные могут быть устаревшими" (это критично, когда сервис формально работает, но показывает неправильное). 

## Итог

Инцидент GitHub (21–22 октября 2018) - показательный пример того, как небольшой инфраструктурный сбой (43 секунды сетевой потери связности) может привести к суточной деградации из-за сложности распределённых систем и требований к целостности данных.
Ключевой управленческий выбор GitHub - сохранить доверие к данным, даже если это увеличивает время восстановления
